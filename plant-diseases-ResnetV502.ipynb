{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def supervised_metrics(y_true, y_pred):\n",
    "    \"\"\"Meterics for a Supervised Learning model:\"\"\"\n",
    "    print(\"Accuracy : {} %\".format(accuracy_score(y_true, y_pred)*100))\n",
    "    print(\"F1 Score : {}\".format(f1_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"Recall : {}\".format(recall_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"Precision : {}\".format(precision_score(y_true, y_pred, average='weighted')))\n",
    "    \n",
    "def view_random_image(root_path,folder,class_folder):\n",
    "    path=root_path+'/'+folder+'/'+class_folder\n",
    "    rand=random.choice(os.listdir(path))\n",
    "    random_image=mpimg.imread(path+'/'+rand)\n",
    "    plt.imshow(random_image)\n",
    "    plt.title(\"File Name: \" + rand)\n",
    "    \n",
    "def pre_process_image(path, image_shape=224, channels=3, norm_factor=255.):\n",
    "    '''Pre-Processing the Image before sending it to the model'''\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=channels)\n",
    "    img = tf.image.resize(img, size = (image_shape, image_shape))\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img = img/norm_factor\n",
    "    return img\n",
    "\n",
    "def random_tester(root_path, classes, model, class_type=\"binary\"):\n",
    "    '''Random Class Folder Selection'''\n",
    "    path=root_path\n",
    "    class_folder=random.choice(os.listdir(path))\n",
    "    \n",
    "    '''Random File Selection'''\n",
    "    folder_path=path+'/'+class_folder+'/'\n",
    "    rand=random.choice(os.listdir(folder_path))\n",
    "    file_path=folder_path+'/'+rand\n",
    "    random_image=mpimg.imread(file_path)\n",
    "    \n",
    "    '''Prediction'''\n",
    "    predicted_value=model.predict(pre_process_image(file_path)) \n",
    "    if(class_type==\"binary\"):\n",
    "        predicted_label=classes[custom_rounder(predicted_value)]\n",
    "    else:\n",
    "        index=tf.math.round(predicted_value).numpy()\n",
    "        index=np.argmax(index)\n",
    "        predicted_label=classes[index]\n",
    "        \n",
    "    '''Visualize'''\n",
    "    plt.imshow(random_image)\n",
    "    plt.title(\"Prediction:\" + predicted_label +\"\\n\" +\"True class: \"+ class_folder)\n",
    "    plt.show()\n",
    "    \n",
    "def loss_curve_plot(df):\n",
    "    \"\"\" Dataframe (df) is history of the fit of the NN model\n",
    "    The df consists of train and validation fit data\n",
    "    \"\"\"\n",
    "    history = df.history\n",
    "    val_accuracy = history[\"val_accuracy\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "    train_accuracy = history[\"accuracy\"]\n",
    "    train_loss = history[\"loss\"]\n",
    "    \n",
    "    \"\"\"Accuracy Plot\"\"\"\n",
    "    plt.plot(train_accuracy, label=\"Train Accuracy\")\n",
    "    plt.plot(val_accuracy, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy Curves\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"Loss Plot\"\"\"\n",
    "    plt.plot(train_loss, label=\"Train loss\")\n",
    "    plt.plot(val_loss, label=\"Validation loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def confusion_matrix_plot(y_true, y_pred, figsize=(30,30)):\n",
    "    \"\"\"\"Confusion Matrix for true values and predicted values\"\"\"\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)\n",
    "    plt.figure(figsize = figsize)\n",
    "    sns.heatmap(cm, annot=True, cmap=\"crest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def supervised_metrics(y_true, y_pred):\n",
    "    \"\"\"Meterics for a Supervised Learning model:\"\"\"\n",
    "    print(\"Accuracy : {} %\".format(accuracy_score(y_true, y_pred)*100))\n",
    "    print(\"F1 Score : {}\".format(f1_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"Recall : {}\".format(recall_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"Precision : {}\".format(precision_score(y_true, y_pred, average='weighted')))\n",
    "    \n",
    "def view_random_image(root_path,folder,class_folder):\n",
    "    path=root_path+'/'+folder+'/'+class_folder\n",
    "    rand=random.choice(os.listdir(path))\n",
    "    random_image=mpimg.imread(path+'/'+rand)\n",
    "    plt.imshow(random_image)\n",
    "    plt.title(\"File Name: \" + rand)\n",
    "    \n",
    "def pre_process_image(path, image_shape=224, channels=3, norm_factor=255.):\n",
    "    '''Pre-Processing the Image before sending it to the model'''\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=channels)\n",
    "    img = tf.image.resize(img, size = (image_shape, image_shape))\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img = img/norm_factor\n",
    "    return img\n",
    "\n",
    "def random_tester(root_path, classes, model, class_type=\"binary\"):\n",
    "    '''Random Class Folder Selection'''\n",
    "    path=root_path\n",
    "    class_folder=random.choice(os.listdir(path))\n",
    "    \n",
    "    '''Random File Selection'''\n",
    "    folder_path=path+'/'+class_folder+'/'\n",
    "    rand=random.choice(os.listdir(folder_path))\n",
    "    file_path=folder_path+'/'+rand\n",
    "    random_image=mpimg.imread(file_path)\n",
    "    \n",
    "    '''Prediction'''\n",
    "    predicted_value=model.predict(pre_process_image(file_path)) \n",
    "    if(class_type==\"binary\"):\n",
    "        predicted_label=classes[custom_rounder(predicted_value)]\n",
    "    else:\n",
    "        index=tf.math.round(predicted_value).numpy()\n",
    "        index=np.argmax(index)\n",
    "        predicted_label=classes[index]\n",
    "        \n",
    "    '''Visualize'''\n",
    "    plt.imshow(random_image)\n",
    "    plt.title(\"Prediction:\" + predicted_label +\"\\n\" +\"True class: \"+ class_folder)\n",
    "    plt.show()\n",
    "    \n",
    "def loss_curve_plot(df):\n",
    "    \"\"\" Dataframe (df) is history of the fit of the NN model\n",
    "    The df consists of train and validation fit data\n",
    "    \"\"\"\n",
    "    history = df.history\n",
    "    val_accuracy = history[\"val_accuracy\"]\n",
    "    val_loss = history[\"val_loss\"]\n",
    "    train_accuracy = history[\"accuracy\"]\n",
    "    train_loss = history[\"loss\"]\n",
    "    \n",
    "    \"\"\"Accuracy Plot\"\"\"\n",
    "    plt.plot(train_accuracy, label=\"Train Accuracy\")\n",
    "    plt.plot(val_accuracy, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy Curves\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"Loss Plot\"\"\"\n",
    "    plt.plot(train_loss, label=\"Train loss\")\n",
    "    plt.plot(val_loss, label=\"Validation loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def confusion_matrix_plot(y_true, y_pred, figsize=(30,30)):\n",
    "    \"\"\"\"Confusion Matrix for true values and predicted values\"\"\"\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)\n",
    "    plt.figure(figsize = figsize)\n",
    "    sns.heatmap(cm, annot=True, cmap=\"crest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70295 images belonging to 38 classes.\n",
      "Found 0 images belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "augmentation=ImageDataGenerator(\n",
    "    rescale=1/225.,\n",
    "    rotation_range=0.3,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "# train_datagen=augmentation.flow_from_directory(\n",
    "#     root+\"/train\",\n",
    "#     target_size=(IMG_SIZE, IMG_SIZE),\n",
    "#     batch_size=32,\n",
    "#     class_mode='categorical',\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "#2025年6月5日缩小模型训练，\n",
    "# 缩小训练集为 100 张图像（示例）\n",
    "train_datagen = augmentation.flow_from_directory(\n",
    "    root + \"/train\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=16,  # 调小 batch 避免显存不足（若用 CPU，batch=8 更稳定）\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training',  # 指定子集（需数据集有 train/valid 子目录）\n",
    ")\n",
    "\n",
    "# test_datagen=augmentation.flow_from_directory(\n",
    "#     root+\"/valid\",\n",
    "#     target_size=(IMG_SIZE, IMG_SIZE),\n",
    "#     batch_size=32,\n",
    "#     class_mode='categorical',\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# 缩小验证集为 50 张图像\n",
    "test_datagen = augmentation.flow_from_directory(\n",
    "    root + \"/valid\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    subset='validation',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mModel: \"functional_3\"\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ resnet50v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">77,862</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                        \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape               \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m        Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (\u001B[38;5;33mInputLayer\u001B[0m)          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)         │               \u001B[38;5;34m0\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ resnet50v2 (\u001B[38;5;33mFunctional\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m2048\u001B[0m)          │      \u001B[38;5;34m23,564,800\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_3           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)                │               \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)                │           \u001B[38;5;34m8,192\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)                │               \u001B[38;5;34m0\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m38\u001B[0m)                  │          \u001B[38;5;34m77,862\u001B[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,650,854</span> (90.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m23,650,854\u001B[0m (90.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81,958</span> (320.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m81,958\u001B[0m (320.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,568,896</span> (89.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m23,568,896\u001B[0m (89.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_base =ResNet50V2(\n",
    "    input_shape=(224,224,3),\n",
    "    include_top=False,\n",
    "    weights=weights_path  # 直接加载工程内的权重文件\n",
    ")\n",
    "model_base.trainable = False\n",
    "\n",
    "#Transfer Learning Model\n",
    "inputs=tf.keras.Input(shape=(224,224,3))\n",
    "x=model_base(inputs)\n",
    "x=lyrs.GlobalAveragePooling2D()(x)\n",
    "x= lyrs.BatchNormalization()(x)\n",
    "x= lyrs.Dropout(0.5)(x)\n",
    "outputs=lyrs.Dense(38, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\DeepLearning\\farmsense.ai-main\\farmsense.ai-main\\tf_env\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m4394/4394\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 595ms/step - accuracy: 0.7468 - loss: 0.9110"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The PyDataset has length 0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m      6\u001B[39m model.compile(loss=\u001B[33m\"\u001B[39m\u001B[33mcategorical_crossentropy\u001B[39m\u001B[33m\"\u001B[39m, optimizer=Adam(learning_rate=\u001B[32m1e-3\u001B[39m, weight_decay=\u001B[32m1e-3\u001B[39m), metrics=[\u001B[33m\"\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# model_history=model.fit(x=train_datagen,\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m#          steps_per_epoch=32,\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m#          validation_data=test_datagen,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m \n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# 减少 Epoch 数：\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_datagen\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_datagen\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 自动计算步数（100/16≈6 步/epoch）\u001B[39;49;00m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest_datagen\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 原 50 改为 10（小数据易过拟合，缩短训练）\u001B[39;49;00m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcheckpointer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Softwares\\DeepLearning\\farmsense.ai-main\\farmsense.ai-main\\tf_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Softwares\\DeepLearning\\farmsense.ai-main\\farmsense.ai-main\\tf_env\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:295\u001B[39m, in \u001B[36mPyDatasetAdapter.get_tf_dataset\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    290\u001B[39m     batches = [\n\u001B[32m    291\u001B[39m         \u001B[38;5;28mself\u001B[39m._standardize_batch(\u001B[38;5;28mself\u001B[39m.py_dataset[i])\n\u001B[32m    292\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_samples)\n\u001B[32m    293\u001B[39m     ]\n\u001B[32m    294\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batches) == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m295\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mThe PyDataset has length 0\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    296\u001B[39m     \u001B[38;5;28mself\u001B[39m._output_signature = data_adapter_utils.get_tensor_spec(batches)\n\u001B[32m    298\u001B[39m ds = tf.data.Dataset.from_generator(\n\u001B[32m    299\u001B[39m     \u001B[38;5;28mself\u001B[39m._get_iterator,\n\u001B[32m    300\u001B[39m     output_signature=\u001B[38;5;28mself\u001B[39m._output_signature,\n\u001B[32m    301\u001B[39m )\n",
      "\u001B[31mValueError\u001B[39m: The PyDataset has length 0"
     ]
    }
   ],
   "source": [
    "# checkpointer = ModelCheckpoint('rice_leaf.hdf5',verbose=1, save_best_only= True)\n",
    "checkpointer = ModelCheckpoint('rice_leaf.keras',verbose=1, save_best_only= True)\n",
    "early_stopping = EarlyStopping(monitor= 'val_loss', patience= 3)\n",
    "\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-3, decay=1e-3), metrics=[\"accuracy\"])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=1e-3, weight_decay=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# model_history=model.fit(x=train_datagen,\n",
    "#          steps_per_epoch=32,\n",
    "#          validation_data=test_datagen,\n",
    "#          epochs=50,\n",
    "#          callbacks=[checkpointer, early_stopping])\n",
    "\n",
    "# 减少 Epoch 数：\n",
    "model.fit(\n",
    "    x=train_datagen,\n",
    "    steps_per_epoch=len(train_datagen),  # 自动计算步数（100/16≈6 步/epoch）\n",
    "    validation_data=test_datagen,\n",
    "    epochs=10,  # 原 50 改为 10（小数据易过拟合，缩短训练）\n",
    "    callbacks=[checkpointer, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels=test_datagen.classes\n",
    "model=tf.keras.models.load_model(\"/kaggle/working/rice_leaf.hdf5\")\n",
    "y_pred=model.predict(test_datagen)\n",
    "prediction=tf.math.round(y_pred).numpy()\n",
    "prediction=prediction.argmax(axis=1)\n",
    "\n",
    "supervised_metrics(labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix_plot(labels, prediction, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}